{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "This final project can be collaborative. The maximum members of a group is 3. You can also work by yourself. Please respect the academic integrity. **Remember: if you get caught on cheating, you get F.**\n",
    "\n",
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task.\n",
    "\n",
    "You only need to complete **TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist**. To cut down training time, we only use a subset of the original dataset (5k out of 20k). The dataset can be found in the same folder. \n",
    "\n",
    "Different from our previous homework, this competition gives you great flexibility (and very few hints). You can freely determine every component of your workflow, including but not limited to:\n",
    "-  **Preprocessing the input text**: You may decide how to clean or transform the text. For example, removing emojis or URLs, lowercasing, removing stopwords, applying stemming or lemmatization, correcting spelling, or performing tokenization and sentence segmentation.\n",
    "-  **Feature extraction and encoding**: You can choose any method to convert text into numerical representations, such as TF-IDF, Bag-of-Words, N-grams, Word2Vec, GloVe, FastText, contextual embeddings (e.g., BERT, RoBERTa, or other transformer-based models), Part-of-Speech (POS) tagging, dependency-based features, sentiment or emotion features, readability metrics, or even embeddings or features generated by large language models (LLMs).\n",
    "-  **Data augmentation and enrichment**: You may expand or balance your dataset by incorporating other related corpora or using techniques like synonym replacement, random deletion/insertion, or LLM-assisted augmentation (e.g., generating paraphrased or synthetic examples to improve model robustness).\n",
    "-  **Model selection**: You are free to experiment with different models ‚Äî from traditional machine learning algorithms (e.g., Logistic Regression, SVM, Random Forest, XGBoost) to deep learning architectures (e.g., CNNs, RNNs, Transformers), or even hybrid/ensemble approaches that combine multiple models or leverage LLM-generated predictions or reasoning.\n",
    "\n",
    "## Requirements\n",
    "-  **Input**: the text for each instance.\n",
    "-  **Output**: the binary label for each instance.\n",
    "-  **Feature engineering**: use at least 2 different methods to extract features and encode text into numerical values. You may explore both traditional and AI-assisted techniques. Data augmentation is optional.\n",
    "-  **Model selection**: implement with at least 3 different models and compare their performance.\n",
    "-  **Evaluation**: create a dataframe with rows indicating feature+model and columns indicating Precision (P), Recall (R) and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. Here is an example illustrating how the experimental results table should be presented.\n",
    "\n",
    "| Feature + Model | Sexist (P) | Sexist (R) | Sexist (F1) | Non-Sexist (P) | Non-Sexist (R) | Non-Sexist (F1) | Weighted (P) | Weighted (R) | Weighted (F1) |\n",
    "|-----------------|:----------:|:----------:|:------------:|:---------------:|:---------------:|:----------------:|:-------------:|:--------------:|:---------------:|\n",
    "| TF-IDF + Logistic Regression | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "- **Format of the report**: add explainations for each step (you can add markdown cells). At the end of the report, write a summary for each sections: \n",
    "    - Data Preprocessing\n",
    "    - Feature Engineering\n",
    "    - Model Selection and Architecture\n",
    "    - Training and Validation\n",
    "    - Evaluation and Results\n",
    "    - Use of Generative AI (if you use)\n",
    "\n",
    "## Rules \n",
    "Violations will result in 0 points in the grade: \n",
    "-   `Rule 1 - No test set leakage`: You must not use any instance from the test set during training, feature engineering, or model selection.\n",
    "-   `Rule 2 - Responsible AI use`: You may use generative AI, but you must clearly document how it was used. If you have used genAI, include a section titled ‚ÄúUse of Generative AI‚Äù describing:\n",
    "    -   What parts of the project you used AI for\n",
    "    -   What was implemented manually vs. with AI assistance\n",
    "\n",
    "## Grading\n",
    "\n",
    "The performance should be only evaluated on the test set (a total of 1086 instances). Please split original dataset into train set and test set. The test set should NEVER be used in the training process. The evaluation metric is a combination of precision, recall, and f1-score (use `classification_report` in sklearn). \n",
    "\n",
    "The total points are 10.0. Each team will compete with other teams in the class on their best performance. Points will be deducted if not following the requirements above. \n",
    "\n",
    "If ALL the requirements are met:\n",
    "- Top 25\\% teams: 10.0 points.\n",
    "- Top 25\\% - 50\\% teams: 8.5 points.\n",
    "- Top 50\\% - 75\\% teams: 7.0 points.\n",
    "- Top 75\\% - 100\\% teams: 6.0 points.\n",
    "\n",
    "If your best performance reaches **0.82** or above (weighted F1-score) and follows all the requirements and rules, you will also get full points (10.0 points). \n",
    "\n",
    "## Submission\n",
    "Similar as homework, submit both a PDF and .ipynb version of the report including: \n",
    "- code and experimental results with details explained\n",
    "- combined results table, report and best performance\n",
    "- a summary at the end of the report (please follow the format above)\n",
    "\n",
    "Missing any part of the above requirements will result in point deductions.\n",
    "\n",
    "The due date is **Dec 11, Thursday by 11:59pm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6156cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordsegment in c:\\users\\tucso\\anaconda3\\lib\\site-packages (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell is for calling all the imports\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sentence_transformers import SentenceTransformer\n",
    "!pip install wordsegment\n",
    "from wordsegment import segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "13f3da4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewire_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sexism2022_english-16993</td>\n",
       "      <td>Then, she's a keeper. üòâ</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexism2022_english-13149</td>\n",
       "      <td>This is like the Metallica video where the poo...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sexism2022_english-13021</td>\n",
       "      <td>woman?</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexism2022_english-14998</td>\n",
       "      <td>Unlicensed day care worker reportedly tells co...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sexism2022_english-7228</td>\n",
       "      <td>[USER] Leg day is easy. Hot girls who wear min...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rewire_id  \\\n",
       "0  sexism2022_english-16993   \n",
       "1  sexism2022_english-13149   \n",
       "2  sexism2022_english-13021   \n",
       "3  sexism2022_english-14998   \n",
       "4   sexism2022_english-7228   \n",
       "\n",
       "                                                text       label  split  \n",
       "0                            Then, she's a keeper. üòâ  not sexist  train  \n",
       "1  This is like the Metallica video where the poo...  not sexist  train  \n",
       "2                                             woman?  not sexist  train  \n",
       "3  Unlicensed day care worker reportedly tells co...  not sexist  train  \n",
       "4  [USER] Leg day is easy. Hot girls who wear min...      sexist  train  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "This cell initializes the random seed loads the data in, shows what it looks like\n",
    "and provides percentages of sexist vs non sexist and train vs test in the data\n",
    "IMPLEMENTED MANUALLY\n",
    "\n",
    "Using expanded training dataset: edos_labelled_data_train_only.csv\n",
    "Test set from original: edos_labelled_data.csv\n",
    "'''\n",
    "RANDOM_SEED = 24\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Load expanded training data\n",
    "train_data_file = \"edos_labelled_data_train_only.csv\"\n",
    "df_train_full = pd.read_csv(train_data_file)\n",
    "\n",
    "# Load original data to get test set (for consistent evaluation)\n",
    "test_data_file = \"edos_labelled_data.csv\"\n",
    "df_original = pd.read_csv(test_data_file)\n",
    "\n",
    "\n",
    "display(df_train_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7d4feddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell contains the function to process the input from the data\n",
    "'''\n",
    "# print(df['text'].head(5))\n",
    "\n",
    "# print(df['label'].value_counts())\n",
    "# print(df.isnull().sum())\n",
    "# print(df.duplicated().sum())\n",
    "\n",
    "def split_hashtag(word):\n",
    "    hashtag = word.group(1)\n",
    "    return segment(hashtag)\n",
    "\n",
    "def preprocess_text(s):\n",
    "    if isinstance(s, str):\n",
    "        s = html.unescape(s)\n",
    "        s = re.sub(r'[^a-zA-Z\\s]', '', s)\n",
    "        s = s.strip()\n",
    "        s = s.lower()\n",
    "        s = re.sub(r'http\\S+|www\\S+', '', s)\n",
    "        s = re.sub(r'@(\\w+)', '', s)\n",
    "        s = re.sub(r'#(\\w+)', split_hashtag, s)\n",
    "        s = re.sub(r'\\s+', ' ', s).strip()\n",
    "        return s\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Preprocess training data\n",
    "df_train_full['clean_text'] = df_train_full['text'].apply(preprocess_text)\n",
    "\n",
    "# Preprocess test data\n",
    "df_original['clean_text'] = df_original['text'].apply(preprocess_text)\n",
    "\n",
    "# print(df['clean_text'].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "23d146b7-b415-46d8-86ba-2c488ed0fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data columns: ['rewire_id', 'text', 'label', 'split', 'clean_text']\n",
      "Test data columns: ['rewire_id', 'text', 'label', 'split', 'clean_text']\n",
      "\n",
      "Training set size: 14000\n",
      "Test set size: 1086\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell separates the training and test sets\n",
    "Training data: from edos_labelled_data_train_only.csv (all rows are train)\n",
    "Test data: from edos_labelled_data.csv (test split only)\n",
    "'''\n",
    "\n",
    "# Training data: all rows from train-only file are training data\n",
    "train_df = df_train_full.copy()\n",
    "\n",
    "# Test data: only test split from original file\n",
    "test_df = df_original[df_original['split'] == 'test'].copy()\n",
    "\n",
    "print(\"Training data columns:\", train_df.columns.tolist())\n",
    "print(\"Test data columns:\", test_df.columns.tolist())\n",
    "print(f\"\\nTraining set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ced15b6a-a231-41cd-9288-f8d1aa02d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Setup the training data for use later on \n",
    "'''\n",
    "# Training Data\n",
    "X_train = train_df['clean_text']\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Test Data\n",
    "X_test = test_df['clean_text']\n",
    "y_test = test_df['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf36b1d-a814-4b68-967a-019abfd68401",
   "metadata": {},
   "source": [
    "## Feature Engineering Method 1 + 3 Models \n",
    "‚Ä¢ Method 1: TF-IDF\n",
    "\n",
    "‚Ä¢ Models: Logistic Regression, Linear SVC, Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7eb04037-cd15-405d-bab4-1d2ab584608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (14000, 20000)\n",
      "Vocabulary Size: 20000\n",
      "Using sublinear_tf=True for better term frequency scaling\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell will set up TfidVectorizer so we can fit the training data\n",
    "and transform the test data\n",
    "\n",
    "Improved TF-IDF settings:\n",
    "- max_features=20000: Limit vocabulary size for efficiency\n",
    "- ngram_range=(1, 3): Includes unigrams, bigrams, and trigrams\n",
    "- min_df=2: Word must appear in at least 2 documents\n",
    "- max_df=0.9: Ignore words that appear in >90% of documents (stopwords)\n",
    "- sublinear_tf=True: Apply log scaling to term frequency (helps with common words)\n",
    "'''\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 3),  # unigrams, bigrams, trigrams\n",
    "    min_df=2,            # word must appear in at least 2 documents\n",
    "    max_df=0.9,          # ignore words in >90% of documents\n",
    "    sublinear_tf=True,   # log scaling for term frequency (improves performance)\n",
    "    norm='l2'            # L2 normalization (default, but explicit)\n",
    ")\n",
    "\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF shape: \" + str(X_train_vect.shape))\n",
    "print(\"Vocabulary Size: \" + str(len(vectorizer.vocabulary_)))\n",
    "print(\"Using sublinear_tf=True for better term frequency scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "220eb382-6e33-4483-9d67-c8c12b2685e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell is for the helper function I will be using to create the new data frame\n",
    "for the final results. Used Generative AI to come up with this helper function\n",
    "for quickly adding the results of models to a list\n",
    "'''\n",
    "results = []\n",
    "\n",
    "def add_result(feature_name, model_name, y_true, y_pred):\n",
    "    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    row = {\"Feature + Model\": feature_name + \" + \" + model_name}\n",
    "\n",
    "    # class labels only (gets Sexist + Non-Sexist labels only)\n",
    "    classes = [c for c in rep if c not in (\"accuracy\", \"macro avg\", \"weighted avg\")]\n",
    "\n",
    "    # per-class metrics (no f-strings)\n",
    "    for c in classes:\n",
    "        metrics = rep[c]\n",
    "        row[c + \" (P)\"] = metrics[\"precision\"]\n",
    "        row[c + \" (R)\"] = metrics[\"recall\"]\n",
    "        row[c + \" (F1)\"] = metrics[\"f1-score\"]\n",
    "\n",
    "    # weighted metrics\n",
    "    weighted = rep[\"weighted avg\"]\n",
    "    row[\"Weighted (P)\"] = weighted[\"precision\"]\n",
    "    row[\"Weighted (R)\"] = weighted[\"recall\"]\n",
    "    row[\"Weighted (F1)\"] = weighted[\"f1-score\"]\n",
    "\n",
    "    results.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "da521595-2184-4b9e-a5c9-99aa8655fcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the TF-IDF + Logistic Regression model!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell trains Logistic Regression model\n",
    "'''\n",
    "\n",
    "logReg = LogisticRegression(max_iter = 500, random_state = RANDOM_SEED)\n",
    "\n",
    "logReg.fit(X_train_vect, y_train)\n",
    "logReg_pred = logReg.predict(X_test_vect)\n",
    "\n",
    "add_result(\"TF-IDF\", \"Logistic Regression\", y_test, logReg_pred)\n",
    "\n",
    "print(\"Trained the TF-IDF + Logistic Regression model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ff37afd1-3101-4136-9824-aae7a87e46d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[225], line 28\u001b[0m\n\u001b[0;32m     19\u001b[0m linSVC_test \u001b[38;5;241m=\u001b[39m LinearSVC(\n\u001b[0;32m     20\u001b[0m     C\u001b[38;5;241m=\u001b[39mC_value,\n\u001b[0;32m     21\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Handle class imbalance\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Cross-validation on training set only\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[0;32m     29\u001b[0m     linSVC_test, \n\u001b[0;32m     30\u001b[0m     X_train_vect, \n\u001b[0;32m     31\u001b[0m     y_train, \n\u001b[0;32m     32\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv, \n\u001b[0;32m     33\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     34\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m mean_f1 \u001b[38;5;241m=\u001b[39m cv_scores\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     38\u001b[0m std_f1 \u001b[38;5;241m=\u001b[39m cv_scores\u001b[38;5;241m.\u001b[39mstd()\n",
      "File \u001b[1;32mc:\\Users\\tucso\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tucso\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    685\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    686\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    687\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    688\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    689\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    690\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    691\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    692\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    693\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    694\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    695\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    696\u001b[0m )\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tucso\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tucso\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    412\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    413\u001b[0m         clone(estimator),\n\u001b[0;32m    414\u001b[0m         X,\n\u001b[0;32m    415\u001b[0m         y,\n\u001b[0;32m    416\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    417\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    418\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    419\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    420\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    421\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    422\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[0;32m    423\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    424\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    425\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    426\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    427\u001b[0m     )\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    429\u001b[0m )\n\u001b[0;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tucso\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\tucso\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\tucso\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tucso\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "TF-IDF + LinearSVC with C parameter tuning and class balancing\n",
    "Uses cross-validation on training set only (no test leakage)\n",
    "\n",
    "class_weight='balanced' automatically adjusts for class imbalance\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "best_C = None\n",
    "best_cv_f1 = 0\n",
    "\n",
    "# Test C values (expanded range)\n",
    "for C_value in [0.1, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0, 5.0, 10.0]:\n",
    "    linSVC_test = LinearSVC(\n",
    "        C=C_value,\n",
    "        max_iter=2000,\n",
    "        random_state=RANDOM_SEED,\n",
    "        #loss='squared_hinge',\n",
    "        class_weight='balanced'  # Handle class imbalance\n",
    "    )\n",
    "    \n",
    "    # Cross-validation on training set only\n",
    "    cv_scores = cross_val_score(\n",
    "        linSVC_test, \n",
    "        X_train_vect, \n",
    "        y_train, \n",
    "        cv=cv, \n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    mean_f1 = cv_scores.mean()\n",
    "    std_f1 = cv_scores.std()\n",
    "    \n",
    "    if mean_f1 > best_cv_f1:\n",
    "        best_cv_f1 = mean_f1\n",
    "        best_C = C_value\n",
    "\n",
    "\n",
    "linSVC_tuned = LinearSVC(\n",
    "    C=best_C,\n",
    "    max_iter=2000,\n",
    "    random_state=RANDOM_SEED,\n",
    "    #loss='squared_hinge',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "linSVC_tuned.fit(X_train_vect, y_train)\n",
    "linSVC_tuned_pred = linSVC_tuned.predict(X_test_vect)\n",
    "\n",
    "add_result(\"TF-IDF\", \"LinearSVC\", y_test, linSVC_tuned_pred)\n",
    "\n",
    "print(\"Trained the TF-IDF + LinearSVC!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f2a19-daa2-4997-b0b2-ede1b60fe22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the TF-IDF + Random Forest!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell trains the Random Forest\n",
    "'''\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 200, random_state = RANDOM_SEED, n_jobs = 1)\n",
    "rf.fit(X_train_vect, y_train)\n",
    "pred_rf = rf.predict(X_test_vect)\n",
    "add_result(\"TF-IDF\", \"Random Forest\", y_test, pred_rf)\n",
    "\n",
    "print(\"Trained the TF-IDF + Random Forest!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca41f4e-edf7-4266-8367-10b4adb88ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d513914cfc4ffa8c7e57afb56b882d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b07ff32f8040358ea52689fb771d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "X_train_sbert = sbert_model.encode(X_train.tolist(), show_progress_bar = True, batch_size = 32)\n",
    "\n",
    "X_test_sbert = sbert_model.encode(X_test.tolist(), show_progress_bar = True, batch_size = 32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccec2ed-c941-4147-9469-a974f4185f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the SBERT + Logistic Regression!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This model trains SBERT + Logistic Regression \n",
    "'''\n",
    "\n",
    "lr_sbert = LogisticRegression(max_iter = 500, random_state = RANDOM_SEED)\n",
    "lr_sbert.fit(X_train_sbert, y_train)\n",
    "lr_sbert_pred = lr_sbert.predict(X_test_sbert)\n",
    "add_result(\"SBERT\", \"Logistic Regression\", y_test, lr_sbert_pred)\n",
    "\n",
    "print(\"Trained the SBERT + Logistic Regression!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe64ab-cfb1-4c49-a967-f367e6a6aa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the SBERT + LinearSVC!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This model trains SBERT + LinearSVC\n",
    "'''\n",
    "\n",
    "linSVC_sbert = LinearSVC(max_iter = 1000, random_state = RANDOM_SEED)\n",
    "linSVC_sbert.fit(X_train_sbert, y_train)\n",
    "linSVC_sbert_pred = linSVC_sbert.predict(X_test_sbert)\n",
    "\n",
    "add_result(\"SBERT\", \"LinearSVC\", y_test, linSVC_sbert_pred)\n",
    "\n",
    "print(\"Trained the SBERT + LinearSVC!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3715c0-df42-467f-8fb8-d21e3b089ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the SBERT + Random Forest!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This model trains SBERT + Random Forest\n",
    "'''\n",
    "\n",
    "rf_sbert = RandomForestClassifier(n_estimators = 200, random_state = RANDOM_SEED, n_jobs = 1)\n",
    "rf_sbert.fit(X_train_sbert, y_train)\n",
    "rf_sbert_pred = rf_sbert.predict(X_test_sbert)\n",
    "\n",
    "add_result(\"SBERT\", \"Random Forest\", y_test, rf_sbert_pred)\n",
    "\n",
    "print(\"Trained the SBERT + Random Forest!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161ad8c-7fbe-431d-9e7b-70431d5cdc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature + Model</th>\n",
       "      <th>not sexist (P)</th>\n",
       "      <th>not sexist (R)</th>\n",
       "      <th>not sexist (F1)</th>\n",
       "      <th>sexist (P)</th>\n",
       "      <th>sexist (R)</th>\n",
       "      <th>sexist (F1)</th>\n",
       "      <th>Weighted (P)</th>\n",
       "      <th>Weighted (R)</th>\n",
       "      <th>Weighted (F1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF + Logistic Regression</td>\n",
       "      <td>0.784057</td>\n",
       "      <td>0.984791</td>\n",
       "      <td>0.873034</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.279461</td>\n",
       "      <td>0.423469</td>\n",
       "      <td>0.808568</td>\n",
       "      <td>0.791897</td>\n",
       "      <td>0.750087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF + LinearSVC</td>\n",
       "      <td>0.872682</td>\n",
       "      <td>0.894804</td>\n",
       "      <td>0.883605</td>\n",
       "      <td>0.700361</td>\n",
       "      <td>0.653199</td>\n",
       "      <td>0.675958</td>\n",
       "      <td>0.825556</td>\n",
       "      <td>0.828729</td>\n",
       "      <td>0.826817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF + Random Forest</td>\n",
       "      <td>0.808687</td>\n",
       "      <td>0.991128</td>\n",
       "      <td>0.890661</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.377104</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.844920</td>\n",
       "      <td>0.823204</td>\n",
       "      <td>0.794341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SBERT + Logistic Regression</td>\n",
       "      <td>0.799136</td>\n",
       "      <td>0.937896</td>\n",
       "      <td>0.862974</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.485777</td>\n",
       "      <td>0.770315</td>\n",
       "      <td>0.783610</td>\n",
       "      <td>0.759818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SBERT + LinearSVC</td>\n",
       "      <td>0.806311</td>\n",
       "      <td>0.939163</td>\n",
       "      <td>0.867681</td>\n",
       "      <td>0.712575</td>\n",
       "      <td>0.400673</td>\n",
       "      <td>0.512931</td>\n",
       "      <td>0.780676</td>\n",
       "      <td>0.791897</td>\n",
       "      <td>0.770664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SBERT + Random Forest</td>\n",
       "      <td>0.733645</td>\n",
       "      <td>0.994930</td>\n",
       "      <td>0.844540</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.076677</td>\n",
       "      <td>0.738118</td>\n",
       "      <td>0.733886</td>\n",
       "      <td>0.634544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Feature + Model  not sexist (P)  not sexist (R)  \\\n",
       "0  TF-IDF + Logistic Regression        0.784057        0.984791   \n",
       "1            TF-IDF + LinearSVC        0.872682        0.894804   \n",
       "2        TF-IDF + Random Forest        0.808687        0.991128   \n",
       "3   SBERT + Logistic Regression        0.799136        0.937896   \n",
       "4             SBERT + LinearSVC        0.806311        0.939163   \n",
       "5         SBERT + Random Forest        0.733645        0.994930   \n",
       "\n",
       "   not sexist (F1)  sexist (P)  sexist (R)  sexist (F1)  Weighted (P)  \\\n",
       "0         0.873034    0.873684    0.279461     0.423469      0.808568   \n",
       "1         0.883605    0.700361    0.653199     0.675958      0.825556   \n",
       "2         0.890661    0.941176    0.377104     0.538462      0.844920   \n",
       "3         0.862974    0.693750    0.373737     0.485777      0.770315   \n",
       "4         0.867681    0.712575    0.400673     0.512931      0.780676   \n",
       "5         0.844540    0.750000    0.040404     0.076677      0.738118   \n",
       "\n",
       "   Weighted (R)  Weighted (F1)  \n",
       "0      0.791897       0.750087  \n",
       "1      0.828729       0.826817  \n",
       "2      0.823204       0.794341  \n",
       "3      0.783610       0.759818  \n",
       "4      0.791897       0.770664  \n",
       "5      0.733886       0.634544  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST MODEL: TF-IDF + LinearSVC\n",
      "Weighted F1: 0.8268\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "creates the final table\n",
    "'''\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "best = None\n",
    "best_f1 = 0\n",
    "\n",
    "for index, row in results_df.iterrows():\n",
    "    if (row['Weighted (F1)'] > best_f1):\n",
    "        best_f1 = row['Weighted (F1)']\n",
    "        best = row\n",
    "\n",
    "print(\"BEST MODEL: \" + best[\"Feature + Model\"])\n",
    "print(\"Weighted F1: \" + format(best[\"Weighted (F1)\"], \".4f\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8675e93",
   "metadata": {},
   "source": [
    "## Experimental Results\n",
    "\n",
    "(A table detailed model performance on the test set with at least 6 rows. Report the best performance.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f84b04",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "\n",
    "### 2. Feature Engineering\n",
    " \n",
    "\n",
    "### 3. Model Selection and Architecture\n",
    "\n",
    "\n",
    "### 4. Training and Validation\n",
    "\n",
    "\n",
    "### 5. Evaluation and Results\n",
    "\n",
    "\n",
    "### 6. Use of Generative AI (if you use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242656d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ace1772",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "Data preprocessing was handled by deciding how we wanted to split up the text and what to remove. At first the plan was to keep the text the same remove unneccessary white space, make it all lower case, remove '@' but keep the usernames, remove '#' but keep the text after. This was an effective strategy that worked at the start. When it came time to optimize we realized that this was not the most effective strategy. We decided to remove everything after an '@' because that identifying username was unlikely to come up again, download wordsegment to split compound words and seperate the text after '#' because each hashtag has important identifiers that go unnoticed if it is stuck in a one word. Another strategy was removing anything that is not a letter because they are do not give anything away. Also split up the data between training and test. We found out that the training dataset didn't contain enough to train our model as much as we wanted so we took the original data of 20k samples and filtered out all of the non-train splits. We then added in all of the training data into edos_labelled_data_train_only.csv and we use that extra data to train the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406443b",
   "metadata": {},
   "source": [
    "## Feature Engineineering\n",
    "The 2 methods that we used to extract features and encode the text into numerical values were TF-IDF and SBERT. TF-IDF was a traditional method and it is known as a good option for short text. To optimize the vectorizer we set max_features to 20000 because that the largest the vocabulary size got to after preprocessing was around 16,000. Also changed the ngram_range from default by  (1,3), this means that 1, 2, and 3 word phrases were added as to the vocabulary. Set the min_df to 2 which means that in order for a word to even appear in the matrix it needs to have appeaed at least twice. Set max_df to .9 which means that if a word appears more than 90% of the time then it must not be taken into account. This is for words like 'a','the', etc. We also applied sublinear_tf = true which applys logarithmic scaling to the term frequency to improve performance. The second method we used was SBERT with sentence transformer. This method usually works better on larger data sets so it was not as effective as TF-IDF was when running on the same models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb53a5",
   "metadata": {},
   "source": [
    "## Model Selection and Architecture\n",
    "\n",
    "Architecture: Raw-Text -> Preprocessing -> Feature Engineerring -> Model -> Prediction\n",
    "\n",
    "For this project we decided to use Logistic Regression, Linear SVC, and Random Forest as our models. We chose the Logistic Regression model because it is a simple model we have used in class and is great for binary classification. Linear SVC was chosen because it is good at dealing with text data and fast for training. Random Forest Classifier was used because it is different from the other 2 and is supposed to be more accurate and stable because of all the decision steps it goes through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a42a7a",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "\n",
    "The dataset we used for training (edos_labelled_data_train_only.csv) is from the orginial 20,000 examples dataset and we used a simple python script to extract all of the train examples. The test set uses the test examples taken from edos_labelled_data.csv. All of the models were trained using .fit() on the training data. When early testing our data we found the TF-IDF + LinearSVC was giving us the best results so we implemented class_weight = 'balanced' (to handle class imbalance) and used cross validation on the training data to tune our C parameter for LinearSVC. The model then choses the best C value based on the weighted F1 score. The final model of LinearSVC was then trained using the optimal C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b612f11",
   "metadata": {},
   "source": [
    "## Evaluation and Results\n",
    "\n",
    "![image.png](Results.png)\n",
    "\n",
    "The best performing combination was the TF-IDF + LinearSVC combination with an overall score 0.826817. I believe that the TF-IDF feature outperformed the SBERT because of the size of the training sets and the examples in each. TF-IDF specializes for short text like the social media remarks while SBERT specializes in the use of sentences and much larger data sets. LinearSVC was the overall best model because it the top model when both features were applied. The models overall seem to do pretty poorly at identifyin sexist remarks which leads to them being exceptionally good at calling out non sexist text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca0782",
   "metadata": {},
   "source": [
    "## Use of Generative AI\n",
    "\n",
    "Generative AI was used to create a rough outline of the steps needed to complete the project and decide on which models we will use for the final project. For preprocessing the text, when it came to hashtags Google's search AI and ChatGPT were used to identify the python libraries that already exist for that purpose and implementing them into the preprocess function. ChatGPT was used to create the function that adds the result of each feature and model and formats it into a table that is easy to read for the final display of results. Finally Google search and its AI feature were used to figure out how to fine tune the models for best results including implementation of Cross-validation and class balancing stratagies (weight_class)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
